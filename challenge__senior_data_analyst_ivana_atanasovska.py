# -*- coding: utf-8 -*-
"""Challenge _Senior_Data_Analyst_Ivana_Atanasovska.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19s1nCpnkv6tkI9SqFnuWU5g50XvMglzH
"""

!pip install ydata-profiling
import numpy as np
import pandas as pd
from ydata_profiling import ProfileReport
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

"""**Read Data**"""

# Define the path to the folder and the file name
file_name = "./interview_analysis_molecule_x_10mg_v1.tsv"

# Combine folder and file name to create the full file path
file_path = os.path.join(file_name)

data = pd.read_csv(file_path, sep='\t', header=0)
data

"""**Exploratory Data Analysis**"""

data.shape

data.info()

# check unique values
data.nunique()

data.describe()

# check missing values
data.isna().sum().sort_values()

# check duplicates
data.duplicated().sum()

"""**Data Understanding**

**contract_id** - unique identifiers for each contract. It's used to distinguish one contract from another in the dataset.

**participants_no:** The number of participants involved in the contract

**participants:** Names or identifiers of the participants involved in the contract

**participants_price:** The price associated with each participant

**winner:** The participant identified as the winner of the contract

**maximum_price_allowed:** The maximum price allowed for the contract

**winner_price:** The price offered by the winner

**published_date** - date when the contract was published.

**published_date_month** - the month when the contract was published. However, it seems to be redundant since published_date column already contains the exact information (and even more accurate) therefore I decided to drop this column.

**start_date** - indicates the start date of each contract.

**end_date_extension** - the original end date of the contract, which has been extended by a certain duration.

**duration_extension** - the duration of the extension of the contracts in months.

**duration** - the duration of the contract in months.

The plot below shows a diverse distribution of buyers, with some buyers being significantly more active or involved in more contracts than others.
"""

# Plot distribution of buyers
plt.figure(figsize=(10,8))
sns.countplot(x='buyer',data=data,palette='rocket_r',order=data['buyer'].value_counts().index)
plt.xticks(rotation=90)
plt.show()

from scipy.stats import pearsonr

# Calculate buyer frequency
buyer_counts = data['buyer'].value_counts().reset_index()
buyer_counts.columns = ['buyer', 'buyer_count']

# Merge buyer frequency with the original data
data = data.merge(buyer_counts, on='buyer')

# Analyze Buyer Impact

# Correlation between buyer frequency and other variables
corr_winner_price = pearsonr(data['buyer_count'], data['winner_price'])
corr_quantity_total = pearsonr(data['buyer_count'], data['quantity_total'])

print(f"Correlation between buyer frequency and winner price: {corr_winner_price[0]:.2f} (p-value: {corr_winner_price[1]:.2f})")
print(f"Correlation between buyer frequency and quantity total: {corr_quantity_total[0]:.2f} (p-value: {corr_quantity_total[1]:.2f})")

# Descriptive statistics for winner_price and quantity_total grouped by buyers
grouped_stats = data.groupby('buyer')[['winner_price', 'quantity_total']].describe()
print(grouped_stats)

"""**Correlation between Buyer Frequency and Winner Price:**
The positive correlation coefficient of 0.22 suggests a weak positive relationship between buyer frequency and winner price, meaning as buyer frequency increases, the winner price tends to slightly increase as well. However, the p-value of 0.24 indicates that this correlation is not statistically significant at common significance levels (e.g., 0.05), so this relationship could be due to random chance.

**Correlation between Buyer Frequency and Quantity Total:**
The positive correlation coefficient of 0.36 indicates a moderate positive relationship between buyer frequency and quantity total, meaning as buyer frequency increases, the total quantity tends to increase as well. The p-value of 0.04 indicates that this correlation is statistically significant at the 0.05 significance level, suggesting that this relationship is likely not due to random chance.
"""

# Plot distribution of Contracts by Region
plt.figure(figsize=(13, 10))
data['region'].value_counts().plot.pie(autopct='%1.1f%%', colors=sns.color_palette('hsv', len(data['region'].unique())))
plt.ylabel('')  # Remove the y-label to clean up the plot
plt.title('Distribution of Contracts by Region')
plt.show()

"""The distribution shows a diverse spread of contracts across multiple regions, with a few regions having significantly higher representation.
Understanding these distributions can help in regional analysis and targeted strategies for areas with more contract activities.
"""

# Plot distribution of winners
sns.countplot(x=data['winner'])

"""participants_16 is the most frequent winner, appearing 16 times in the dataset.

participants_19 is the second most frequent winner, appearing 12 times.

participants_6 and participants_23 have won significantly fewer contracts, appearing only 2 times and 1 time respectively.
"""

# Plot Total Quantity Sold in Each Region by Winner
plt.figure(figsize=(12,10))
plt.title('Total Quantity Sold in Each Region by Winner')
sns.barplot(x='region',y='quantity_total',data=data,hue='winner',order=data['region'].value_counts().index,palette='rocket')
plt.xlabel('Region',fontsize=15)
plt.show()

"""This plot shows the distribution of total quantities sold across different regions and how it varies among different winners."""

# Plot contract type distribution by region
plt.figure(figsize=(12, 8))
sns.countplot(x='region', hue='contract_type', data=data, order=data['region'].value_counts().index, palette='rocket')
plt.title('Contract Type Distribution by Region')
plt.xlabel('Region', fontsize=15)
plt.ylabel('Count of Contracts', fontsize=15)
plt.legend(title='Contract Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""The above plot shows the distribution of different contract types across various regions. This visualization helps to understand how different regions prefer different contract types helping to identify any regional preferences or patterns in contract types."""

# Plot the distribution of each numerical variable
data.hist(figsize=(8,8),bins=50)
plt.show()

"""Visualizing the distribution of each numerical variable helps to understand the distribution of data, identifying skewness, and the presence of outliers.

**Data Preprocessing**
"""

# Convert date columns to datetime type
date_cols = ['published_date', 'start_date', 'end_date_extension', 'published_date_month']
for col in date_cols:
    data[col] = pd.to_datetime(data[col])

# Convert quantity_total to integer after rounding to three decimal places
data['quantity_total'] = data['quantity_total'].round(3).astype(int)

"""*Analyzing the target variable*"""

# Plot distribution of winner_price
plt.figure(figsize=(10, 6))
sns.histplot(data['winner_price'], bins=10, kde=True)
plt.title('Distribution of Winner Price')
plt.xlabel('Winner Price')
plt.ylabel('Frequency')
plt.show()

data['winner_price'].describe()

# Detect and handle outliers using Z-score
from scipy import stats

z_scores = np.abs(stats.zscore(data['winner_price']))
outliers_zscore = data[(z_scores > 3)]
outliers_zscore

# Use IQR to detect outliers
Q1 = data['winner_price'].quantile(0.25)
Q3 = data['winner_price'].quantile(0.75)
IQR = Q3 - Q1
outliers_iqr = data[((data['winner_price'] < (Q1 - 1.5 * IQR)) | (data['winner_price'] > (Q3 + 1.5 * IQR)))]

# Remove outliers identified by the IQR method
data = data[~((data['winner_price'] < (Q1 - 1.5 * IQR)) | (data['winner_price'] > (Q3 + 1.5 * IQR)))]

# Summary statistics for winner_price after removing IQR outliers
cleaned_winner_price_summary_iqr = data['winner_price'].describe()
cleaned_winner_price_summary_iqr

# Plot distribution of winner_price after removing outliers
plt.figure(figsize=(10, 6))
sns.histplot(data['winner_price'], bins=10, kde=True)
plt.title('Distribution of Winner Price after removing outliers')
plt.xlabel('Winner Price')
plt.ylabel('Frequency')
plt.show()

"""*Outlier Detection and Removal* :

I used the statistical methods Z-score and IQR to detect and handle outliers. Based on the Z-score Method, no outliers were detected (all Z-scores <= 3).Based on IQR Method, 9 potential outliers were identified. I decided to remove the outliers identified by the IQR method because they deemed invalid or anomalous.


*Comparison Before and After Cleaning* :

The overall distribution appears more consistent and robust for use as a target variable in a machine learning model.

No negative prices found in the dataset.
No missing values in the winner_price column.
I ensured that the winner_price column contains only numerical values.

*Check if the target variable "**winner_price**" is indeed the mininum one by comparing the values from "**minimum participants_price**"*
"""

# Compare "winner_price" to minimum participants_price


# Function to get the minimum participant price
def get_min_participant_price(prices):
    price_list = [float(price) for price in prices.split('|')]
    return min(price_list)

# Apply the function to calculate the minimum price for each row
data['min_participant_price'] = data['participants_price'].apply(lambda x: get_min_participant_price(x) if pd.notnull(x) else None)

# Compare winner_price with min_participant_price
data['price_match'] = data.apply(lambda row: row['winner_price'] == row['min_participant_price'] if pd.notnull(row['min_participant_price']) else False, axis=1)

# Identify discrepancies
discrepancies = data[~data['price_match']]

# Show summary of discrepancies
discrepancies_summary = discrepancies[['contract_id', 'winner_price', 'participants_price', 'min_participant_price']]

# Save discrepancies to a new file for review
discrepancies_summary.to_csv('discrepancies_summary.tsv', sep='\t', index=False)


print(discrepancies_summary)

# Findings summary
total_rows = data.shape[0]
discrepant_rows = discrepancies.shape[0]
discrepant_percentage = (discrepant_rows / total_rows) * 100

print(f'Total Rows: {total_rows}')
print(f'Discrepant Rows: {discrepant_rows}')
print(f'Discrepant Percentage: {discrepant_percentage:.2f}%')

# Visualize discrepancies
plt.figure(figsize=(10, 6))
sns.scatterplot(x='winner_price', y='min_participant_price', data=data)
plt.title('Winner Price vs. Minimum Participant Price')
plt.xlabel('Winner Price')
plt.ylabel('Minimum Participant Price')
plt.show()

"""The winner_price column is equal to the minimum price offered in the participants_price column.

*Column Cleaning*
"""

data[['contract_id','participants_no','participants', 'participants_price','winner', 'maximum_price_allowed','winner_price']]

"""After I checked that the **winner_price** column is equal to the minimum price offered in the **participants_price** column, I decided to drop the **participants** and the **participants_price** columns since I don't need them anymore. This way I reduce the dataset dimensionality because we don't need redundant data. However, we could also keep the **participants_price** column for further analysis to better understanding the pricing associated with each participant for analysis of bidding behavior and pricing strategies. Analyzing this variable could reveal trends in pricing across different participants or contracts.

Also, I have decided to drop the **second_place_outcome**, **second_place**, and **second_place_price** columns as they have 15 missing values from a total of 30. This also reduces the dataset dimensionality.

Columns with constant values, such as: '**atc**', '**sku**', '**active_ingredient**' and '**pack_strength**' have been dropped as well. If a variable has only one unique value (i.e., a constant value), it does not provide any variability in the data and therefore does not contribute to the analysis.

"""

# Drop the unnecessary columns
unnecessary_columns = ['participants', 'participants_price','second_place_outcome', 'second_place', 'second_place_price','min_participant_price', 'buyer_count']
data = data.drop(columns=unnecessary_columns)

# Identify and drop constant columns
constant_columns = [col for col in data.columns if data[col].nunique() == 1]
data = data.drop(columns=constant_columns)
print(data.shape)

"""*Analyzing the maximum_price_allowed column*"""

data['maximum_price_allowed']

# Plot distribution of Maximum Price Allowed
plt.figure(figsize=(10, 6))
sns.histplot(data['maximum_price_allowed'], bins=10, kde=True)
plt.title('Distribution of Maximum Price Allowed')
plt.xlabel('Maximum Price Allowed')
plt.ylabel('Frequency')
plt.show()

data['maximum_price_allowed'].describe()

"""The column '**maximum_price_allowed**' has outlier values, I have decided to drop the values higher than the 90th percentile as those values differ a lot. However, it is worth mentioning that with only 22 samples and 3 outliers, removing outliers can significantly impact statistical measures such as means, standard deviations, and correlations.

"""

threshold = data['maximum_price_allowed'].quantile(0.9)

print("The 90th percentile value is:", threshold)

# Filter out rows where column1 exceeds the 90th percentile threshold
data = data[data['maximum_price_allowed'] < threshold]

# Plot distribution of Maximum Price Allowed after removing outliers
plt.figure(figsize=(10, 6))
sns.histplot(data['maximum_price_allowed'], bins=10, kde=True)
plt.title('Distribution of Maximum Price Allowed After Removing the Outliers')
plt.xlabel('Maximum Price Allowed')
plt.ylabel('Frequency')
plt.show()

data.shape

data.info()

"""*Analyzing the timeline, duration, and any extensions of the contracts.*"""

data[['contract_id','published_date', 'published_date_month', 'start_date', 'end_date_extension', 'duration_extension', 'duration']]

# Calculate the total duration and the difference in months
data['months_between_start_and_extended_date'] = (data['end_date_extension'] - data['start_date']).dt.days // 30
data['total_duration'] = data['duration_extension'] + data['duration']
data['difference'] = data['months_between_start_and_extended_date'] - data['total_duration']


data[['contract_id','published_date', 'published_date_month', 'start_date', 'end_date_extension', 'duration_extension', 'duration', 'months_between_start_and_extended_date','total_duration', 'difference']]

"""With this analysis I wanted to extract the **total_duration** of a contract measured in months. For this I used **start_date** and **end_date_extention** columns. Also, I used the **total_duration** metric to check the correcntess of the **duration** and **duration_extention** columns and see if there is any errors associated with the dates and/or the duration columns. No problems regarding this were found in the dataset."""

# Find rows where published_date is after start_date
invalid_dates = data[data['published_date'] > data['start_date']]

# Display invalid rows
print("\nRows where published_date is after start_date:")
print(invalid_dates)

"""With this analysis I wanted to check if **published_date** happened before the **start_date** of the contract, as this seems logicall and natural to me. However, I identified 6 cases when **published_date** is after **start_date**.

Observations where the **published_date** falls after the **start_date** could potentially indicate inconsistencies within the dataset. Generally, I expect the **published_date** to be before the **start_date** in contract records. This misalignment might stem due to data entry errors.

I chose not to drop these rows because I'm uncertain whether these instances represent errors. I'll need further expertise and clarification on these variables to determine if they are indeed connected and if any discrepancies are present.

Based on the analysis above, it seems that **published_date_month** can be dropped. This column seem redundant, as it is derived from **published_date** and may not offer additional information beyond what's already captured in **published_date**.
"""

col_to_be_dropped = ['published_date_month','months_between_start_and_extended_date', 'difference', 'total_duration' ]
data = data.drop(columns=col_to_be_dropped)

"""*Analysing the columns "quantity_annual" and the "quantity_total"*"""

data[['quantity_annual', 'quantity_total']]

print("Describe quantity_annual:")
print(data['quantity_annual'].describe())

print("\nDescribe quantity_total:")
print(data['quantity_total'].describe())

# Calculate correlation matrix
corr_matrix = data[['quantity_annual', 'quantity_total', 'winner_price']].corr()

# Plot the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

"""*High Correlation between **quantity_annual**  and **quantity_total** :

The correlation coefficient between **quantity_annual** and **quantity_total** is 0.92, indicating a strong positive correlation. This suggests that these two variables are highly related and one can be used to infer the other.

*Moderate Negative Correlation with **winner_price**:*

Both **quantity_annual** and **quantity_total** have moderate negative correlations with **winner_price**, with coefficients of -0.21 and -0.18 respectively. This suggests that as the quantity increases, the **winner_price** tends to decrease slightly, which could be due to economies of scale or competitive pricing for larger quantities.

*Suggestions Based on the Correlation Matrix*:

Given the high correlation between **quantity_annual** and **quantity_total**, it might be redundant to keep both columns in the dataset. Since **quantity_total** represents the total quantity over the entire contract duration, it might be more informative in the context of contract analysis. Therefore, I decided to drop **quantity_annual**.


High correlation between **quantity_annual** and **quantity_total** is not unexpected, and **quantity_total** is an acqumulative metric including the quantity annual.

I believe **quantity_total** provides comprehensive information about the total quantity throughout the contract duration, and considering the uncertainty surrounding the interpretation of **quantity_annual**, I decided to drop the **quantity_annual** variable from further analysis. By doing so, we focus on the most informative variables thereby enhancing the clarity and robustness of our analysis.
"""

data = data.drop(columns = 'quantity_annual')

data.info()

data

"""**Encoding the Dataset**

Encoding the dataset is essential to convert categorical variables into a numerical format, enabling machine learning algorithms to process and learn from the data effectively. This transformation helps in improving model performance and accuracy.
"""

from sklearn.preprocessing import LabelEncoder


# Determine categorical columns dynamically
categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()

# Initialize data_encoded as a copy of data
data_encoded = data.copy()

# Label encode categorical columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    data_encoded[col] = le.fit_transform(data[col].astype(str))
    label_encoders[col] = le
print(data_encoded.head())

# Ensure all columns are numeric
print(data_encoded.dtypes)

# Calculate correlation matrix
corr_matrix = data_encoded.corr()

# Plot the correlation matrix
plt.figure(figsize=(14, 12))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Focus on the correlation with the target variable 'winner_price'
print(corr_matrix['winner_price'].sort_values(ascending=False))

data_encoded.shape

"""**Buyer** and **region** variables are correlated, which is not unexpected as this suggests that there is some relationship between the buyers and the regions in which they operate

Exploratory Data Analysis (EDA) can be further automized in a very efficient way by using ydata profiling. This library standardizes the generation of detailed reports, complete with statistics and visualizations. In this way the Data Scientist can quicky ensure the data quality and readiness for further analysis.
"""

from ydata_profiling import ProfileReport

ProfileReport(data_encoded)

"""**Split Dataset**"""

# Split the dataset into X & y
y = data_encoded['winner_price']
x_df = data_encoded.drop(columns='winner_price')

x_df.head()

"""**Scaling the Data**

Scaling the data is a crucial step before feeding it into a machine learning model, especially for algorithms that are sensitive to the scale of input features such as linear regression, support vector machines, and neural networks.

By scaling the appropriate columns, we ensure that our model can train more effectively and produce more reliable results.
"""

from sklearn.preprocessing import StandardScaler

# Columns to be scaled
columns_to_scale = ['duration_extension', 'duration', 'participants_no', 'quantity_total', 'maximum_price_allowed']

# Initialize the StandardScaler
scaler = StandardScaler()

# Scale the selected columns
x_df[columns_to_scale] = scaler.fit_transform(x_df[columns_to_scale])

x_df.head()

"""**Split the dataset into training and testing sets for model evaluation**"""

from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets (80/20 split for example)
X_train, X_test, y_train, y_test = train_test_split(x_df, y, test_size=0.2, random_state=42)

print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

"""#**Summary**

#What manual tasks would you perform?

Scaling from a few SKUs to hundreds of thousands involves automating many processes, but some manual tasks and strategic oversight are still necessary to ensure the quality and success of the project. Here are the key manual tasks I would perform:

**Data Preparation and Exploration**

- Data Schema Design: Define a robust schema to handle the large-scale dataset, including types, constraints, and relationships between tables.

- Initial Data Cleaning: Perform an initial thorough data cleaning to handle missing values, outliers, and inconsistencies. This often requires domain expertise and cannot be fully automated.

- Exploratory Data Analysis (EDA): Conduct a detailed EDA to understand the data distribution, identify key patterns, and uncover potential issues. This includes visualizing data trends, checking for seasonality, and understanding correlations.

**Feature Engineering**

- Feature Selection and Creation: Manually select and create features that are likely to be relevant for the models. This involves domain knowledge to understand which features could be predictive and why.

- Domain-Specific Transformations: Implement domain-specific transformations and aggregations that might not be apparent to automated systems.
Model Development

- Model Selection: Manually select and experiment with different types of models (e.g., linear regression, decision trees, neural networks) based on the problem and data characteristics.

- Feature Importance Analysis: Manually inspect feature importance and correlations to ensure that the selected features make sense and are not introducing bias.

**Evaluation and Validation**

- Model Validation: Manually review the model validation results to ensure the model is performing well across different segments of the data. This includes checking for overfitting and underfitting.

- Hyperparameter Tuning: While hyperparameter tuning can be automated, initially setting ranges and interpreting the results requires manual intervention.

**Deployment and Monitoring**

- Deployment Strategy: Design a deployment strategy, including the architecture for model serving, rollback procedures, and versioning.

- Monitoring and Maintenance: Set up manual checks for monitoring model performance and data drift over time. Regularly review these metrics and update models as necessary.

# What automated approaches can you use?

Automating processes is crucial when scaling from a few SKUs to hundreds of thousands, especially for tasks that are repetitive, data-intensive, and require consistent execution. Here are some automated approaches we can use for different aspects of the Machine Learning modeling and exploratory data analysis (EDA) workflow:

**Data Preparation and Exploration**

*Automated Data Cleaning:*

- Missing Value Imputation
- Outlier Detection and Removal

*Automated EDA:*

- YData Profiling (example provided above)
- Sweetviz

**Feature Engineering**

*Automated Feature Engineering:*

- Feature Selection with Automated Tools: for example, SelectKBest from sklearn.feature_selection


**Model Development**

*Automated Model Training and Selection:*

- AutoML Tools: H2O.ai, AutoML, TPOT

*Hyperparameter Tuning:*

- GridSearchCV and RandomizedSearchCV

**Deployment and Monitoring**

*Automated Model Deployment:*

- CI/CD Pipelines (e.g., GitHub Actions, Jenkins)

#How would you improve this process long term and how would you build your roadmap?

**Long-term Improvements**

To improve the process long-term and ensure scalability, efficiency, and robustness, it's essential to establish a structured approach that incorporates automation, modularity, and continuous monitoring.

- Automate the Entire Pipeline: We can use workflow orchestration tools like Apache Airflow to automate data loading, preprocessing, feature engineering, model training, and evaluation.
- Schedule periodic runs to ensure the pipeline processes new data as it becomes available.
- Break down the pipeline into reusable functions for each step: data loading, cleaning, encoding, scaling, splitting, and modeling.
- Use distributed data processing frameworks like Apache Spark or Dask for large datasets.
- Maintain comprehensive documentation of the pipeline, including code comments, README files, and user guides.
"""